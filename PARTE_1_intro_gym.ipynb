{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnID4yguIeX7"
      },
      "source": [
        "# Introducción a Gymnasium en google colab\n",
        "\n",
        "## Comentarios\n",
        "\n",
        "Este notebook ha sido adaptado del material original de la clase T81-558: Applications of Deep Neural Networks, Module 12: Reinforcement Learning, del\n",
        "Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7l5rd2MIeX8"
      },
      "source": [
        "Otros notebooks del mismo autor sobre RL, cuyos enlaces originales son:\n",
        "\n",
        "* **Part 12.1: Introduction to Introduction to Gymnasium** [[Video]](https://www.youtube.com/watch?v=FvuyrpzvwdI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_1_reinforcement.ipynb)\n",
        "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=VKuqvbG_KAw&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_2_qlearningreinforcement.ipynb)\n",
        "* Part 12.3: Stable Baselines Q-Learning [[Video]](https://www.youtube.com/watch?v=kl7zsCjULN0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_3_pytorch_reinforce.ipynb)\n",
        "* Part 12.4: Atari Games with Stable Baselines Neural Networks [[Video]](https://www.youtube.com/watch?v=maLA1_d4pzQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_4_atari.ipynb)\n",
        "* Part 12.5: Future of Reinforcement Learning [[Video]](https://www.youtube.com/watch?v=-euo5pTjP8E&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_5_rl_future.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UImTzmGTIeX9"
      },
      "source": [
        "# 1 - Introducción a Gymnasium\n",
        "\n",
        "[Gymnasium](https://github.com/Farama-Foundation/Gymnasium) tiene como objetivo proporcionar un punto de referencia de inteligencia general fácil de configurar con diversos entornos. El objetivo es estandarizar la definición de entornos en las publicaciones de investigación en IA para facilitar la reproducción de la investigación publicada. El proyecto afirma ofrecer al usuario una interfaz sencilla. Gymnasium es una bifurcación de OpenAI Gym, para el cual OpenAI dejó de dar soporte en octubre de 2021. Actualmente, Gymnasium cuenta con el soporte de [The Farama Foundation](https://farama.org/).\n",
        "\n",
        "Gymnasium se instala mediante pip en su equipo local. Hay algunas limitaciones importantes que debe tener en cuenta:\n",
        "\n",
        "* Gymnasium Atari solo es compatible **directamente** con Linux y Macintosh\n",
        "* Gymnasium Atari se puede utilizar con Windows; Sin embargo, requiere un procedimiento de instalación específico (https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30).\n",
        "* Gymnasium no puede renderizar juegos animados directamente en Google CoLab.\n",
        "\n",
        "Dado que Gymnasium requiere una pantalla gráfica, la única forma de visualizar Gymnasium en Google CoLab es mediante un vídeo incrustado. La presentación de las animaciones del juego Gymnasium en Google CoLab se describe más adelante en este módulo.\n",
        "\n",
        "## Análisis de los entornos de Gymnasium\n",
        "\n",
        "El componente central de Gymnasium es el entorno, que define el \"juego\" en el que competirá el algoritmo de refuerzo. Un entorno no tiene por qué ser un juego; sin embargo, describe las siguientes características similares a las de un juego:\n",
        "* **Espacio de acción**: ¿Qué acciones podemos realizar en el entorno en cada paso/episodio para modificarlo?\n",
        "* **Espacio de observación**: ¿Cuál es el estado actual de la parte del entorno que podemos observar? Normalmente, podemos ver el entorno completo.\n",
        "\n",
        "Antes de comenzar a analizar Gymnasium, es fundamental comprender la terminología que utiliza esta biblioteca.\n",
        "\n",
        "* **Agente**: El programa o modelo de aprendizaje automático que controla las acciones.\n",
        "Paso: Una ronda de acciones que afectan al espacio de observación.\n",
        "* **Episodio**: Un conjunto de pasos que finaliza cuando el agente no cumple el objetivo del entorno o el episodio alcanza el número máximo de pasos permitidos.\n",
        "* **Renderizado**: Gymnasium puede renderizar un fotograma para su visualización después de cada episodio.\n",
        "* **Recompensa**: Un refuerzo positivo que puede ocurrir al final de cada episodio, después de que el agente actúe.\n",
        "* **No determinista**: En algunos entornos, la aleatoriedad es un factor que determina el efecto de las acciones en la recompensa y los cambios en el espacio de observación.\n",
        "\n",
        "Gymnasium debe instalarse con el siguiente comando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5IJAsJACn5z",
        "outputId": "7083192d-04db-42fc-b169-917c2b33422f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "\u001b[33mWARNING: gymnasium 1.2.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[accept-rom-license,atari]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUZEqaiSNTlU",
        "outputId": "86a2850e-734e-47f3-aeaa-10bc27950bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/dist-packages (from ale-py) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub31e1EnCea7"
      },
      "source": [
        "Es importante tener en cuenta que muchos entornos Gymnasium especifican que no son no deterministas, aunque utilizan números aleatorios para procesar acciones. Según el issue tracker de biblioteca Gymnasium en GitHub, una propiedad no determinista significa que un entorno determinista se comporta de forma aleatoria. Incluso asignando al entorno un valor de semilla consistente, este comportamiento se confirma.\n",
        "\n",
        "El programa puede usar el método de semilla de un entorno (`env.seed`) para generar el generador de números aleatorios correspondiente.\n",
        "\n",
        "La biblioteca Gymnasium permite consultar algunos de estos atributos de los entornos. Creé la siguiente función para consultar los entornos Gymnasium."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-zsGlmStKcwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "def query_environment(name):\n",
        "    env = gym.make(name)\n",
        "    spec = gym.spec(name)\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space}\")\n",
        "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "# Access reward_range from the spec object if it exists\n",
        "    if hasattr(spec, 'reward_range'):\n",
        "        print(f\"Reward Range: {spec.reward_range}\")\n",
        "    else:\n",
        "        print(\"Reward Range: Not specified in environment spec\")\n",
        "\n",
        "    if hasattr(spec, 'reward_threshold'):\n",
        "         print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "    else:\n",
        "        print(\"Reward Threshold: Not specified in environment spec\")"
      ],
      "metadata": {
        "id": "oLfgGEgXKdRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRXfAdwFDwUm"
      },
      "source": [
        "Analizaremos el entorno **MountainCar-v0**, que desafía a un coche de baja potencia a escapar del valle entre dos montañas. El siguiente código describe el entorno Mountian Car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYwy9cjlJjEH",
        "outputId": "375d4783-da55-4875-a485-942593b7c63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(3)\n",
            "Observation Space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: Not specified in environment spec\n",
            "Reward Threshold: -110.0\n"
          ]
        }
      ],
      "source": [
        "query_environment(\"MountainCar-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TsQiaGJE3UA"
      },
      "source": [
        "Este entorno permite tres acciones distintas: acelerar hacia adelante, desacelerar o retroceder. El espacio de observación contiene dos valores continuos (de punto flotante), como lo demuestra el objeto de caja. El espacio de observación es simplemente la posición y la velocidad del coche. El coche tiene 200 pasos para escapar en cada episodio. Habría que consultar el código, pero el coche de montaña no recibe ninguna recompensa incremental. La única recompensa para el vehículo se obtiene al escapar del valle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF4n5cYEMyru",
        "outputId": "63f6cbac-e100-4744-f21e-63ca8db80f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Max Episode Steps: 500\n",
            "Nondeterministic: False\n",
            "Reward Range: Not specified in environment spec\n",
            "Reward Threshold: 475.0\n"
          ]
        }
      ],
      "source": [
        "query_environment(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwvVKrNebUHJ"
      },
      "source": [
        "El entorno **CartPole-v1** reta al agente a equilibrar un poste mientras el agente. También pueden conocer este problema con el nombre de péndulo invertido. El entorno tiene un espacio de observación de 4 números continuos:\n",
        "\n",
        "* Posición del carro\n",
        "* Velocidad del carro\n",
        "* Ángulo del poste\n",
        "* Velocidad del poste en la punta\n",
        "\n",
        "Para lograr este objetivo, el agente puede realizar las siguientes acciones:\n",
        "\n",
        "* Empujar el carro hacia la izquierda\n",
        "* Empujar el carro hacia la derecha\n",
        "\n",
        "También existe una variante continua del carro de montaña. Esta versión no solo tiene el motor encendido o apagado. El espacio de acción es un único número de punto flotante para el carro continuo que especifica cuánta fuerza hacia adelante o hacia atrás utiliza actualmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAlaMcJmNSY0",
        "outputId": "ed67fb84-81c8-4c77-9278-59234495f1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Box(-1.0, 1.0, (1,), float32)\n",
            "Observation Space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Max Episode Steps: 999\n",
            "Nondeterministic: False\n",
            "Reward Range: Not specified in environment spec\n",
            "Reward Threshold: 90.0\n"
          ]
        }
      ],
      "source": [
        "query_environment(\"MountainCarContinuous-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liyNcsthtSFp"
      },
      "source": [
        "Gymnasium ofrece una plataforma versátil para desarrollar y comparar algoritmos de aprendizaje por refuerzo. Es compatible con una amplia gama de entornos, incluyendo juegos clásicos de Atari, a través del emulador Arcade Learning Environment (ALE). Esta integración permite a investigadores y entusiastas acceder a una colección de videojuegos retro diseñados originalmente para la consola Atari 2600, utilizándolos como referencia para el rendimiento de la IA. Al interactuar con ALE, los usuarios de Gymnasium pueden implementar fácilmente sus algoritmos y probarlos frente a los desafíos que presentan estos juegos. Cada juego presenta escenarios únicos que ayudan a entrenar algoritmos para que aprendan diversas tareas, lo que convierte a Gymnasium en una herramienta invaluable para el avance de la inteligencia artificial a través de estos entornos interactivos y complejos.\n",
        "\n",
        "Los algoritmos de aprendizaje por refuerzo (RL) pueden recibir información de un juego de Atari de dos maneras principales, que se adaptan a diferentes aspectos del estado y la complejidad del juego.\n",
        "\n",
        "El primer método consiste en monitorizar la pantalla del juego o la salida visual que genera. En este enfoque, el algoritmo de RL procesa los píxeles de la pantalla del juego como el estado de su entorno. Esto es similar a cómo un jugador humano vería e interpretaría el juego. El algoritmo analiza los patrones, movimientos y cambios dentro de los fotogramas para tomar decisiones sobre la mejor acción a realizar en cada paso. Este método requiere que el modelo de aprendizaje por retroalimentación (RL) maneje datos de alta dimensión y aprenda a asociar las señales visuales con los resultados del juego.\n",
        "\n",
        "El segundo método consiste en monitorizar la RAM del sistema Atari. A pesar de su capacidad limitada, la RAM de un sistema Atari contiene toda la información sobre el estado interno del juego, como la ubicación de los objetos, las puntuaciones de los jugadores y el estado del juego. Al acceder directamente a esta memoria, un algoritmo de aprendizaje por retroalimentación (RL) puede acceder a una representación más compacta y con menos ruido del estado del juego que la que proporcionan los datos de píxeles. Esto puede ser beneficioso para un aprendizaje más eficiente, ya que el estado del sistema se representa de forma más estructurada y con menos dimensiones.\n",
        "\n",
        "Ambos métodos tienen sus ventajas. El enfoque de captura de pantalla obliga al algoritmo a aprender directamente de la información visual, lo cual es un enfoque más general y más cercano a cómo los humanos juegan. Por otro lado, el método de monitorización de RAM puede resultar en tiempos de entrenamiento más rápidos y, potencialmente, en una comprensión más profunda de la mecánica del juego, ya que evita la necesidad de interpretar datos visuales. La elección entre estos métodos depende de los objetivos y las limitaciones específicas del proyecto de aprendizaje directo en cuestión.\n",
        "\n",
        "Primero, veamos cómo monitorizar la pantalla del juego [Breakout](https://ale.farama.org/environments/breakout/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndTb-9pgJizW",
        "outputId": "f4208ad1-deb8-4cd8-df24-7441bc05ac20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
            "Max Episode Steps: None\n",
            "Nondeterministic: False\n",
            "Reward Range: Not specified in environment spec\n",
            "Reward Threshold: None\n"
          ]
        }
      ],
      "source": [
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "env = gym.make('ALE/Breakout-v5')\n",
        "query_environment(\"ALE/Breakout-v5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzeW4zwuCTH"
      },
      "source": [
        "De manera similar, podemos observar Breakout RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni1rxzmLKAdH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "546663ec-48ed-4e10-a5a1-2eeeb622821c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameNotFound",
          "evalue": "Environment `Breakout-ram` doesn't exist in namespace ALE. Did you mean: `Breakout`?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-794492792.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALE/Breakout-ram-v5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-502000199.py\u001b[0m in \u001b[0;36mquery_environment\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action Space: {env.action_space}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;31m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0menv_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menv_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         raise error.Error(\n\u001b[1;32m    528\u001b[0m             \u001b[0;34mf\"No registered env with id: {env_name}. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\" Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;34mf\"Environment `{name}` doesn't exist{namespace_msg}.{suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     )\n",
            "\u001b[0;31mNameNotFound\u001b[0m: Environment `Breakout-ram` doesn't exist in namespace ALE. Did you mean: `Breakout`?"
          ]
        }
      ],
      "source": [
        "query_environment(\"ALE/Breakout-ram-v5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E253PBGPRuw"
      },
      "source": [
        "# 2-  Render OpenAI Gym Environments from CoLab\n",
        "\n",
        "It is possible to visualize the game your agent is playing, even on CoLab. This section provides information on generating a video in CoLab that shows you an episode of the game your agent is playing. I based this video process on suggestions found [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t).\n",
        "\n",
        "Begin by installing **pyvirtualdisplay** and **python-opengl**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF92FCzZMWPn"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "!pip install pyvirtualdisplay\n",
        "!sudo apt-get install -y xvfb ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS7L8kFMLkjN"
      },
      "source": [
        "Next, we install the needed requirements to display an Atari game."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U renderlab"
      ],
      "metadata": {
        "id": "XsYcqc-9P0GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[all]\n",
        "!pip install moviepy  # Para procesamiento de video\n",
        "!apt update &> /dev/null\n",
        "!apt install xvfb &> /dev/null  # Para renderizado sin pantalla\n",
        "!apt install python-opengl &> /dev/null\n",
        "!apt install ffmpeg &> /dev/null  # Para codificación de video"
      ],
      "metadata": {
        "id": "7fbzKQBaQZIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recordvideo tutorial"
      ],
      "metadata": {
        "id": "gpCB195BTqHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0IgKgII_TsSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import numpy as np\n",
        "\n",
        "# Crear el ambiente base\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Envolver con RecordVideo\n",
        "# video_folder: carpeta donde se guardarán los videos\n",
        "# episode_trigger: función que decide cuándo grabar (por defecto cada 10 episodios)\n",
        "env = RecordVideo(\n",
        "    env,\n",
        "    video_folder=\"./videos\",\n",
        "    episode_trigger=lambda episode_id: episode_id % 10 == 0,  # Grabar cada 10 episodios\n",
        "    name_prefix=\"cartpole-training\"\n",
        ")\n",
        "\n",
        "# Entrenar un agente simple (random)\n",
        "num_episodes = 50\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Política random para demostración\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    print(f\"Episodio {episode}: Reward = {total_reward}\")\n",
        "\n",
        "env.close()\n",
        "print(\"Videos guardados en la carpeta ./videos\")"
      ],
      "metadata": {
        "id": "U35inIb-Tslm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T3OZpubbULmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "# Display the video\n",
        "list_of_files = glob.glob('videos/*.mp4')\n",
        "\n",
        "# Check if any video files were generated\n",
        "if list_of_files:\n",
        "    video_path = list_of_files[0]\n",
        "    video = io.open(video_path, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''\n",
        "        <video width=\"640\" height=\"480\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "    '''.format(encoded.decode('ascii'))))\n",
        "else:\n",
        "    print(\"No video file was generated.\")"
      ],
      "metadata": {
        "id": "A5w2u1_0UL5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## notebook original setup video"
      ],
      "metadata": {
        "id": "fXJ4d4b5T212"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjTHm2SpLz10"
      },
      "source": [
        "Note, the above cell may request to restart the runtime, if this occurs, please restart the CoLab runtime. Next, we define the functions used to show the video by adding it to the CoLab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NATj-kNADT"
      },
      "source": [
        "Now we are ready to play the game.  We use a simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YNuidnZ6CKv"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gymnasium as gymnasium\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "# from renderlab import Recorder\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "# Start virtual display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# Create environment\n",
        "#\n",
        "env = gymnasium.make('ALE/Breakout-v5',render_mode=\"rgb_array\")\n",
        "directory = './video'\n",
        "env = Recorder(env, directory)\n",
        "# env = Recorder(env, frame_rate=30)\n",
        "\n",
        "# env = gymnasium.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
        "# env.metadata['render_fps'] = 30\n",
        "# Reset the environment\n",
        "env.reset()\n",
        "\n",
        "# Setup the wrapper to record the video\n",
        "# Record every episode\n",
        "video_callable=lambda episode_id: True\n",
        "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
        "\n",
        "# Run the environment until done\n",
        "terminated = False\n",
        "truncated = False\n",
        "while not (terminated or truncated):\n",
        "    action = env.action_space.sample()  # replace with your own policy!\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22ZzUGW4Q2Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the video\n",
        "list_of_files = glob.glob('videos/*.mp4')\n",
        "\n",
        "# Check if any video files were generated\n",
        "if list_of_files:\n",
        "    video_path = list_of_files[0]\n",
        "    video = io.open(video_path, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''\n",
        "        <video width=\"640\" height=\"480\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "    '''.format(encoded.decode('ascii'))))\n",
        "else:\n",
        "    print(\"No video file was generated.\")"
      ],
      "metadata": {
        "id": "OAXBQNNoQzfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuxXMsPB8aya"
      },
      "source": [
        "You will note that the **step** and **reset** functions return several values:\n",
        "\n",
        "* **observation** (ObsType): An element of the environment's observation_space as the next observation due to the agent actions. An example is a numpy array containing the positions and velocities of the pole in CartPole.\n",
        "\n",
        "* **reward** (SupportsFloat): The reward as a result of taking the action.\n",
        "\n",
        "* **terminated** (bool): Whether the agent reaches the terminal state (as defined under the MDP of the task) which can be positive or negative. An example is reaching the goal state or moving into the lava from the Sutton and Barton, Gridworld. If true, the user needs to call reset().\n",
        "\n",
        "* **truncated** (bool): Whether the truncation condition outside the scope of the MDP is satisfied. Typically, this is a timelimit, but could also be used to indicate an agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached. If true, the user needs to call reset().\n",
        "\n",
        "* **info** (dict): Contains auxiliary diagnostic information (helpful for debugging, learning, and logging). This might, for instance, contain: metrics that describe the agent's performance state, variables that are hidden from observations, or individual reward terms that are combined to produce the total reward."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9 (torch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}